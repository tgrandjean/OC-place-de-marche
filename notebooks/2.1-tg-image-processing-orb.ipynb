{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Place de marché\n",
    "==============\n",
    "\n",
    "![logo](../reports/figures/logo.png)\n",
    "\n",
    "\n",
    "### Votre mission\n",
    "Votre mission est de **réaliser une première étude de faisabilité d'un moteur de classification** d'articles basé sur une image et une description pour l'automatisation de l'attribution de la catégorie de l'article.\n",
    "\n",
    "Pour ce faire, vous allez **évaluer la possibilité d'extraire des données depuis l'API Amazon** en **prenant connaissance de la documentation** et en **écrivant la requête** qui vous permettrait d'extraire des données supplémentaires. Vous vous assurerez ainsi que vous pourrez bien disposer de plus de données et diversifier les sources de données pour éviter les biais pour votre moteur de classification.\n",
    "\n",
    "Ensuite, vous **analyserez le jeu de données** déjà constitué en **réalisant un prétraitement** des images et des descriptions des produits, une **réduction de dimension**, puis un **clustering**. Les résultats du clustering seront présentés sous la forme d’une représentation en deux dimensions à déterminer, qui ’illustrera le fait que les caractéristiques extraites permettent de regrouper des produits de même catégorie.\n",
    "\n",
    "La représentation graphique vous aidera à convaincre Linda que cette approche de modélisation permettra bien de regrouper des produits de même catégorie.\n",
    "\n",
    "### Contraintes\n",
    "\n",
    "Linda vous a communiqué les contraintes suivantes :\n",
    "\n",
    "   * Limiter le nombre d’articles pris par l’API (par exemple : 1000 lignes) et filtrer sur un unique type d’article (par exemple un type d’article peu présent dans votre échantillon de données actuelles).\n",
    "   * Afin d’extraire les features, mettre en œuvre a minima un algorithme de type SIFT / ORB / SURF.\n",
    "   * Un algorithme de type CNN Transfer Learning peut éventuellement être utilisé en complément, s’il peut apporter un éclairage supplémentaire à la démonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN, KMeans, MiniBatchKMeans\n",
    "from sklearn import metrics\n",
    "    \n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.6)\n",
    "\n",
    "Image.MAX_IMAGE_PIXELS = 93680329\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histogram(image, figsize=(12, 8), kde=False):\n",
    "    image = np.array(image)\n",
    "    if len(image.shape) > 2:\n",
    "        # RGB mode\n",
    "        fig, axes = plt.subplots(3, 1, figsize=figsize)\n",
    "        for channel, color, ax in  zip(range(3), ['r', 'g', 'b'], axes):\n",
    "            sns.distplot(image[:, :, channel].flatten(), \n",
    "                         kde=kde, color=color, ax=ax)\n",
    "    else:\n",
    "        # Gray\n",
    "        sns.distplot(image.flatten(), kde=kde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('../data/raw/'):\n",
    "    if len(filenames) == 1:\n",
    "        df = pd.read_csv(os.path.join(dirname, filenames[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['path'] = df['image'].apply(lambda x: os.path.join('../data/raw/Images/', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = [\n",
    "    'uniq_id',\n",
    "    'crawl_timestamp',\n",
    "    'product_url',\n",
    "    'pid',\n",
    "    'discounted_price',\n",
    "    'is_FK_Advantage_product',\n",
    "    'product_rating',\n",
    "    'overall_rating',\n",
    "    'product_specifications',\n",
    "    'brand',\n",
    "    \n",
    "]\n",
    "df.drop(columns=col_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_level(tree_str, level=-1, strict=False):\n",
    "    \"\"\"return a specific level from product_category_tree.\n",
    "    tips: specify a negative index to access latest part of the tree.\n",
    "    \"\"\"\n",
    "    tree_str = eval(tree_str)[0]\n",
    "    levels = tree_str.split('>>')\n",
    "    levels = list(map(lambda x: x.strip(), levels))\n",
    "    if strict:\n",
    "        return levels[level]\n",
    "    else:\n",
    "        try:\n",
    "            return levels[level]\n",
    "        except IndexError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère le premier niveau de l'arbre des catégories comme label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['product_category_tree'].apply(extract_level, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_down(image, factor=5):\n",
    "    width, height = image.size\n",
    "    target_width, target_height = width // factor, height // factor\n",
    "    return image.resize((target_width, target_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [scale_down(Image.open(x), factor=5) for x in df['path']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size = 10\n",
    "# f, axes = plt.subplots(size, size, figsize=(12, 12))\n",
    "# for ax, im in zip(axes.flatten(), random.sample(images, size ** 2)):\n",
    "#     ax.imshow(im, cmap='gray', aspect='auto')\n",
    "#     ax.set_xticks([])\n",
    "#     ax.set_yticks([])  # to hide tick values on X and Y axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = cv2.ORB_create()\n",
    "\n",
    "def features(image, extractor):\n",
    "    assert type(image) == np.ndarray\n",
    "    keypoints, descriptors = extractor.detectAndCompute(image, None)\n",
    "    return keypoints, descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(images[1], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp, desc = features(np.array(images[1]), extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(cv2.drawKeypoints(np.array(images[1]), kp, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageOps.equalize(images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(ImageOps.equalize(images[1]), kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp, desc = features(np.array(ImageOps.equalize(images[1])), extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(cv2.drawKeypoints(np.array(images[1]), kp, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp, desc = features(np.array(images[1].filter(ImageFilter.BoxBlur(1))), extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(cv2.drawKeypoints(np.array(images[1]), kp, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image):\n",
    "    image = ImageOps.equalize(image)\n",
    "    image = image.filter(ImageFilter.BoxBlur(1))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(images[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp, desc = features(np.array(preprocess(images[1])), extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(cv2.drawKeypoints(np.array(preprocess(images[1])), kp, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 5\n",
    "\n",
    "kp1, desc1 = features(np.array(images[index]), extractor)\n",
    "Image.fromarray(cv2.drawKeypoints(np.array(images[index]), kp1, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 11\n",
    "\n",
    "kp2, desc2 = features(np.array(preprocess(images[index])), extractor)\n",
    "Image.fromarray(cv2.drawKeypoints(np.array(preprocess(images[index])), kp2, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher_create(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "matches = bf.match(desc1, desc2)\n",
    "marches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "Image.fromarray(cv2.drawMatches(np.array(images[5]), kp1,\n",
    "                np.array(images[11]), kp2, matches[:10], flags=2, outImg=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premier essai : peut-on séparer les montres des tasses à café?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_mugs = df[df['label'] == 'Coffee Mugs']\n",
    "coffee_mugs = [scale_down(Image.open(x)) for x in coffee_mugs['path'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watches = df[df['label'] == 'Wrist Watches']\n",
    "watches = [scale_down(Image.open(x)) for x in watches['path'].to_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffee_mugs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1, desc1 = features(np.array(watches[0]), extractor)\n",
    "Image.fromarray(cv2.drawKeypoints(np.array(watches[0]), kp1, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_list = list()\n",
    "for im in watches + coffee_mugs:\n",
    "    im = im.convert('L')\n",
    "    kp, desc = features(np.array(im), extractor)\n",
    "    if (desc is not None):\n",
    "        descriptor_list.append(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(descriptor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(watches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_list = np.concatenate(descriptor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = MiniBatchKMeans(n_clusters=800, init_size=3000)\n",
    "kmeans.fit(descriptor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_res = pca.fit_transform(descriptor_list)\n",
    "pca_res = pd.DataFrame(pca_res)\n",
    "pca_res['kmeans'] = kmeans.labels_\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "# px.scatter_3d(data_frame=pca_res, x=0, y=1, z=2, color='kmeans')\n",
    "sns.scatterplot(data=pca_res, x=0, y=1, hue='kmeans', ax=ax)\n",
    "ax.legend_.remove()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bin_edges = np.histogram(descriptor_list, bins=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bin_edges[:-1], hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_histogram(descriptor, kmeans):\n",
    "    labels = kmeans.predict(descriptor)\n",
    "#     centers = kmeans.cluster_centers_\n",
    "    return Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_images = []\n",
    "for image in watches + coffee_mugs:\n",
    "    image = image.convert('L')\n",
    "    key, desc = features(np.array(image), extractor)\n",
    "    if (desc is not None):\n",
    "        histogram = build_histogram(desc, kmeans)\n",
    "        preprocessed_images.append(histogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bofvw = pd.DataFrame.from_records(preprocessed_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bofvw.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bofvw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=20)\n",
    "pca_50 = pca.fit_transform(bofvw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_50.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "tsne_res = tsne.fit_transform(bofvw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_res = pd.DataFrame(tsne_res)\n",
    "tsne_res['label']= 'watch'\n",
    "tsne_res.loc[149:, 'label'] = 'Mugs'\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "# px.scatter_3d(data_frame=pca_res, x=0, y=1, z=2, color='kmeans')\n",
    "sns.scatterplot(data=tsne_res, x=0, y=1, hue='label', ax=ax)\n",
    "# ax.legend_.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**warning** A ce stade rien ne va plus...\n",
    "\n",
    "Les features extraites sont communes aux deux catégories...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'où vient l'erreur? \n",
    "\n",
    "   * Algorithme très bon mais pas adapté à ce genre de tâches:\n",
    "       * Bon pour créer des photos panoramiques\n",
    "       * Bon pour détecter le même objet dans des conditions différentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = dict()\n",
    "for dirname, _, filenames in os.walk('../data/external/example_lafayette/'):\n",
    "    if filenames:\n",
    "        key = dirname.split('/')[-1]\n",
    "        key = key.replace('\\\\', '-')\n",
    "        items[key] = [Image.open(os.path.join(dirname, x)) for x in filenames]\n",
    "        items[key] = [scale_down(im, factor=2) for im in items[key]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items['watches-1'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1, desc1 = features(np.array(items['watches-1'][0]), extractor)\n",
    "kp2, desc2 = features(np.array(items['watches-1'][1]), extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(cv2.drawKeypoints(np.array(items['watches-1'][0]), kp1, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(cv2.drawKeypoints(np.array(items['watches-1'][1]), kp2, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf = cv2.BFMatcher_create(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "matches = bf.match(desc1, desc2)\n",
    "marches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "Image.fromarray(cv2.drawMatches(np.array(items['watches-1'][0]), kp1,\n",
    "                                np.array(items['watches-1'][1]), kp2, matches[:30], flags=2, outImg=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "matches = flann.knnMatch(np.float32(desc1), np.float32(desc2), k=3)\n",
    "\n",
    "# Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0,0] for i in range(len(matches))]\n",
    "\n",
    "# ratio test as per Lowe's paper\n",
    "for i, x in enumerate(matches):\n",
    "    m, n, o = x\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        matchesMask[i]=[1,0]\n",
    "\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = (255,0,0),\n",
    "                   matchesMask = matchesMask,\n",
    "                   flags=0)\n",
    "\n",
    "Image.fromarray(cv2.drawMatchesKnn(np.array(items['watches-1'][0]), kp1,\n",
    "                                   np.array(items['watches-1'][1]), kp2,\n",
    "                                   matches, None, **draw_params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1, desc1 = features(np.array(watches[0]), extractor)\n",
    "Image.fromarray(cv2.drawKeypoints(np.array(watches[0]), kp1, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp2, desc2 = features(np.array(watches[1]), extractor)\n",
    "Image.fromarray(cv2.drawKeypoints(np.array(watches[1]), kp2, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "matches = flann.knnMatch(np.float32(desc1), np.float32(desc2), k=3)\n",
    "\n",
    "# Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0,0] for i in range(len(matches))]\n",
    "\n",
    "# ratio test as per Lowe's paper\n",
    "for i, x in enumerate(matches):\n",
    "    m, n, o = x\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        matchesMask[i]=[1,0]\n",
    "\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = (255,0,0),\n",
    "                   matchesMask = matchesMask,\n",
    "                   flags=0)\n",
    "\n",
    "Image.fromarray(cv2.drawMatchesKnn(np.array(watches[0]), kp1,\n",
    "                                   np.array(watches[1]), kp2,\n",
    "                                   matches, None, **draw_params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1, desc1 = features(np.array(watches[0]), extractor)\n",
    "Image.fromarray(cv2.drawKeypoints(np.array(watches[0]), kp1, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp2, desc2 = features(np.array(coffee_mugs[0]), extractor)\n",
    "Image.fromarray(cv2.drawKeypoints(np.array(coffee_mugs[0]), kp2, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "matches = flann.knnMatch(np.float32(desc1), np.float32(desc2), k=3)\n",
    "\n",
    "# Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0,0] for i in range(len(matches))]\n",
    "\n",
    "# ratio test as per Lowe's paper\n",
    "for i, x in enumerate(matches):\n",
    "    m, n, o = x\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        matchesMask[i]=[1,0]\n",
    "\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = (255,0,0),\n",
    "                   matchesMask = matchesMask,\n",
    "                   flags=0)\n",
    "\n",
    "Image.fromarray(cv2.drawMatchesKnn(np.array(watches[0]), kp1,\n",
    "                                   np.array(coffee_mugs[0]), kp2,\n",
    "                                   matches, None, **draw_params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp1, desc1 = features(np.array(coffee_mugs[0]), extractor)\n",
    "Image.fromarray(cv2.drawKeypoints(np.array(coffee_mugs[0]), kp1, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp2, desc2 = features(np.array(coffee_mugs[1]), extractor)\n",
    "Image.fromarray(cv2.drawKeypoints(np.array(coffee_mugs[1]), kp2, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 0\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)   # or pass empty dictionary\n",
    "\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "matches = flann.knnMatch(np.float32(desc1), np.float32(desc2), k=3)\n",
    "\n",
    "# Need to draw only good matches, so create a mask\n",
    "matchesMask = [[0,0] for i in range(len(matches))]\n",
    "\n",
    "# ratio test as per Lowe's paper\n",
    "for i, x in enumerate(matches):\n",
    "    m, n, o = x\n",
    "    if m.distance < 0.7*n.distance:\n",
    "        matchesMask[i]=[1,0]\n",
    "\n",
    "draw_params = dict(matchColor = (0,255,0),\n",
    "                   singlePointColor = (255,0,0),\n",
    "                   matchesMask = matchesMask,\n",
    "                   flags=0)\n",
    "\n",
    "Image.fromarray(cv2.drawMatchesKnn(np.array(coffee_mugs[0]), kp1,\n",
    "                                   np.array(coffee_mugs[1]), kp2,\n",
    "                                   matches, None, **draw_params))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.array(coffee_mugs[1]) * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.array(coffee_mugs[0]) * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(watches[0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(coffee_mugs[0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(images[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.array(images[1]) * 3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.array(images[1]) * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "place-de-marche",
   "language": "python",
   "name": "place-de-marche"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
