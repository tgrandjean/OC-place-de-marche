{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "sns.set(font_scale=1.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('../data/raw/'):\n",
    "    if len(filenames) == 1:\n",
    "        df = pd.read_csv(os.path.join(dirname, filenames[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = [\n",
    "    'uniq_id',\n",
    "    'crawl_timestamp',\n",
    "    'product_url',\n",
    "    'pid',\n",
    "    'discounted_price',\n",
    "    'is_FK_Advantage_product',\n",
    "    'product_rating',\n",
    "    'overall_rating',\n",
    "    'product_specifications',\n",
    "    'brand',\n",
    "    \n",
    "]\n",
    "df.drop(columns=col_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_level(tree_str, level=-1, strict=False):\n",
    "    \"\"\"return a specific level from product_category_tree.\n",
    "    tips: specify a negative index to access latest part of the tree.\n",
    "    \"\"\"\n",
    "    tree_str = eval(tree_str)[0]\n",
    "    levels = tree_str.split('>>')\n",
    "    levels = list(map(lambda x: x.strip(), levels))\n",
    "    if strict:\n",
    "        # if strict raise IndexError if the level does'nt exist.\n",
    "        return levels[level]\n",
    "    else:\n",
    "        # otherwise return None if the level does'nt exist.\n",
    "        try:\n",
    "            return levels[level]\n",
    "        except IndexError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lev_1'] = df['product_category_tree'].apply(extract_level, level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lev_1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['product_category_tree'].apply(extract_level, level=2).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['product_category_tree'].apply(extract_level, level=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word tk'] = df['description'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['description'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = re.compile(r'[.,;!?:()/&-]+')\n",
    "sentences = list(map(str.lower, sentences))\n",
    "sentences = [re.sub(punctuation, ' ', x) for x in sentences]\n",
    "\n",
    "# remove numeric data\n",
    "numeric = re.compile(r'\\d+')\n",
    "sentences = [re.sub(numeric, '', x) for x in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = list(map(nltk.word_tokenize, sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = list()\n",
    "for sentence in tokens:\n",
    "    sentence_ = list()\n",
    "    for word in sentence:\n",
    "        if word not in nltk.corpus.stopwords.words('english'):\n",
    "            sentence_.append(word)\n",
    "    filtered.append(sentence_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "token_stem = list()\n",
    "for desc in tokens:\n",
    "    desc_ = list()\n",
    "    for token in desc:\n",
    "        desc_.append(stemmer.stem(token))\n",
    "    token_stem.append(desc_)\n",
    "tokens = token_stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_of_words = []\n",
    "\n",
    "for tk in tokens:\n",
    "    bag_of_words.append(Counter(tk))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bofw = pd.DataFrame.from_records(bag_of_words)\n",
    "df_bofw.fillna(0, inplace=True)\n",
    "df_bofw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bofw.sum(axis=0)[df_bofw.sum(axis=0) < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_bofw.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_bofw.sum(axis=0) / df_bofw.sum(axis=0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_res = PCA(n_components=50).fit_transform(df_bofw)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=10, n_iter=2500)\n",
    "tsne_res = tsne.fit_transform(pca_res)\n",
    "\n",
    "tsne_res = pd.DataFrame(tsne_res)\n",
    "tsne_res['label'] = df['lev_1']\n",
    "tsne_res['product_name'] = df['product_name']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_frame=tsne_res, x=0, y=1, color='label', hover_name='product_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce point il a des mots tokens trop frÃ©quents et d'autre trop peu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as frequencies\n",
    "df_bofw = df_bofw.div(df_bofw.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bofw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bofw.describe().loc['max', :].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bofw.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_bofw.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "too_frequent = df_bofw.sum(axis=0)[df_bofw.sum(axis=0) > 10].index\n",
    "too_rare = df_bofw.sum(axis=0)[df_bofw.sum(axis=0) < 2e-2].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tk in list(too_frequent) + list(too_rare):\n",
    "    df_bofw.drop(tk, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df_bofw.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bofw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_res = PCA(n_components=200).fit_transform(df_bofw)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=10, n_iter=2500)\n",
    "tsne_res = tsne.fit_transform(pca_res)\n",
    "\n",
    "tsne_res = pd.DataFrame(tsne_res)\n",
    "tsne_res['label'] = df['lev_1']\n",
    "tsne_res['product_name'] = df['product_name']\n",
    "\n",
    "px.scatter(data_frame=tsne_res, x=0, y=1, color='label', hover_name='product_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = tsne_res.groupby('label').mean()\n",
    "centers.reset_index(drop=False, inplace=True)\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(24, 18))\n",
    "palette = sns.color_palette(None, centers.shape[0])\n",
    "for i, center in enumerate(centers['label']):\n",
    "    if tsne_res.groupby('label').count().loc[center, 0] > 10:\n",
    "        ax.scatter(x=tsne_res.loc[tsne_res['label'] == center, 0],\n",
    "           y=tsne_res.loc[tsne_res['label'] == center, 1],\n",
    "           color=palette[i]\n",
    "          )\n",
    "        \n",
    "        ax.annotate(center, centers.set_index('label').loc[center, :].values,\n",
    "                    color=palette[i]\n",
    "                   )\n",
    "# plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_res = PCA(n_components=50).fit_transform(df_bofw)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=10, n_iter=2500)\n",
    "tsne_res = tsne.fit_transform(df_bofw)\n",
    "\n",
    "tsne_res = pd.DataFrame(tsne_res)\n",
    "tsne_res['label'] = df['lev_1']\n",
    "tsne_res['product_name'] = df['product_name']\n",
    "\n",
    "centers = tsne_res.groupby('label').mean()\n",
    "centers.reset_index(drop=False, inplace=True)\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(24, 18))\n",
    "palette = sns.color_palette(None, centers.shape[0])\n",
    "for i, center in enumerate(centers['label']):\n",
    "    if tsne_res.groupby('label').count().loc[center, 0] > 10:\n",
    "        ax.scatter(x=tsne_res.loc[tsne_res['label'] == center, 0],\n",
    "           y=tsne_res.loc[tsne_res['label'] == center, 1],\n",
    "           color=palette[i]\n",
    "          )\n",
    "        \n",
    "        ax.annotate(center, centers.set_index('label').loc[center, :].values,\n",
    "                    color=palette[i]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_res = PCA(n_components=2).fit_transform(df_bofw)\n",
    "\n",
    "pca_res = pd.DataFrame(pca_res)\n",
    "pca_res['label'] = df.label\n",
    "\n",
    "centers = pca_res.groupby('label').mean()\n",
    "centers.reset_index(drop=False, inplace=True)\n",
    "\n",
    "_, ax = plt.subplots(1, figsize=(24, 18))\n",
    "palette = sns.color_palette(None, centers.shape[0])\n",
    "for i, center in enumerate(centers['label']):\n",
    "    if pca_res.groupby('label').count().loc[center, 0] > 10:\n",
    "        ax.scatter(x=tsne_res.loc[tsne_res['label'] == center, 0],\n",
    "           y=pca_res.loc[tsne_res['label'] == center, 1],\n",
    "           color=palette[i]\n",
    "          )\n",
    "        \n",
    "        ax.annotate(center, centers.set_index('label').loc[center, :].values,\n",
    "                    color=palette[i]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(df_bofw)\n",
    "\n",
    "fit, ax = plt.subplots(1, figsize=(12, 8))\n",
    "skplt.decomposition.plot_pca_component_variance(pca, ax=ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=.4, min_samples=5)\n",
    "db.fit(tsne_res.iloc[:, :-3])\n",
    "\n",
    "labels = db.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_res['db_group'] = db.labels_.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(data_frame=tsne_res, x=0, y=1, color='db_group', hover_name='product_name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation dans TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "df['path'] = df['image'].apply(lambda x: os.path.join('../data/raw/Images/', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def images_to_sprite(data):\n",
    "        \"\"\"Creates the sprite image along with any necessary padding\n",
    "\n",
    "        Args:\n",
    "          data: NxHxW[x3] tensor containing the images.\n",
    "\n",
    "        Returns:\n",
    "          data: Properly shaped HxWx3 image with any necessary padding.\n",
    "        \"\"\"\n",
    "        if len(data.shape) == 3:\n",
    "            data = np.tile(data[...,np.newaxis], (1,1,1,3))\n",
    "        data = data.astype(np.float32)\n",
    "        min = np.min(data.reshape((data.shape[0], -1)), axis=1)\n",
    "        data = (data.transpose(1,2,3,0) - min).transpose(3,0,1,2)\n",
    "        max = np.max(data.reshape((data.shape[0], -1)), axis=1)\n",
    "        data = (data.transpose(1,2,3,0) / max).transpose(3,0,1,2)\n",
    "        # Inverting the colors seems to look better for MNIST\n",
    "        #data = 1 - data\n",
    "\n",
    "        n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "        padding = ((0, n ** 2 - data.shape[0]), (0, 0),\n",
    "                (0, 0)) + ((0, 0),) * (data.ndim - 3)\n",
    "        data = np.pad(data, padding, mode='constant',\n",
    "                constant_values=0)\n",
    "        # Tile the individual thumbnails into an image.\n",
    "        data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3)\n",
    "                + tuple(range(4, data.ndim + 1)))\n",
    "        data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "        data = (data * 255).astype(np.uint8)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins import projector\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# transform bag of words in tensorflow's data\n",
    "tf_data = tf.Variable(df_bofw.values, name='features')\n",
    "\n",
    "LOG_DIR = '../reports/tf/sessions/text/'\n",
    "\n",
    "for dirname, _, filenames in os.walk(LOG_DIR):\n",
    "    for filename in filenames:\n",
    "        os.remove(os.path.join(dirname, filename))\n",
    "\n",
    "metadata = 'df_labels.tsv'\n",
    "\n",
    "\n",
    "# prepare sprites\n",
    "images = np.array([np.array(Image.open(x).resize((200, 200))) for x in df['path']])\n",
    "sprite = images_to_sprite(images)\n",
    "cv2.imwrite(os.path.join(LOG_DIR, 'sprite_4_classes.png'), sprite)\n",
    "\n",
    "# save the metadata file\n",
    "df['db_group'] = tsne_res['db_group']\n",
    "# df['product_name'].to_csv(os.path.join(LOG_DIR, metadata), index=False, header=False)\n",
    "\n",
    "# save complementary metadate in another file\n",
    "# can't load metadata file with header at starting\n",
    "df[['product_name', 'label', 'db_group']].to_csv(os.path.join(LOG_DIR, metadata), \n",
    "                                                 index=False, header=True, sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# old style from tf <= 2\n",
    "# fake a session to create checkpoint\n",
    "# finally add projector to the session writer.\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver([tf_data])\n",
    "    sess.run(tf_data.initializer)\n",
    "    saver.save(sess, os.path.join(LOG_DIR, 'tf_data.ckpt'))\n",
    "    config = projector.ProjectorConfig()\n",
    "    \n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = tf_data.name\n",
    "    \n",
    "    embedding.sprite.image_path = 'sprite_4_classes.png'\n",
    "    embedding.sprite.single_image_dim.extend([images.shape[1], images.shape[1]])\n",
    "\n",
    "    embedding.metadata_path = metadata\n",
    "    \n",
    "    projector.visualize_embeddings(tf.summary.FileWriter(LOG_DIR), \n",
    "                                   config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word to vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(tokens, min_count=1, size=50, workers=3, window=3, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('key', 'featur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('watch', 'analog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('key', 'watch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('key', 'analog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.get_vector('key')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA et NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "tfid_vectorizer = TfidfVectorizer(max_df=0.95,\n",
    "                                  min_df=2,\n",
    "                                  max_features=no_features,\n",
    "                                  stop_words='english')\n",
    "tfid = tfid_vectorizer.fit_transform(sentences)\n",
    "tfid_feature_names = tfid_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 50\n",
    "\n",
    "nmf = NMF(n_components=no_topics, alpha=.1, l1_ratio=.5, init='nndsvd')\n",
    "nmf.fit(tfid)\n",
    "\n",
    "no_to_words = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic {}:\".format(topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf, tfid_feature_names, no_to_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "place-de-marche",
   "language": "python",
   "name": "place-de-marche"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "1022.2px",
    "left": "2168.67px",
    "right": "20px",
    "top": "84px",
    "width": "588.542px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
